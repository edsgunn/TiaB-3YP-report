% Chapter Template

\chapter{Model} % Main chapter title

\label{Chapter4} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Introduction}


The problem of generation of a set of chords from a melody is very similar to that of translating one language to another. 
The translation problem is one that is very popular in machine learning research and thus there is many resources on it.  
However the music related problem is harder to solve due to the extra dimension each of its elements contains. 
Each element in a melody has both pitch, represented by a descrete symbol or note, and duration whereas each element in the sequence of language is composed of only the discrete symbols or letters.  
Therefore in order to use techniques developed for natural language processing it is necessary to encode the melody and chords in such a way that their dimensions are collapsed into one. 

\section{Related Work}

MySong \\
BLSTMS guy \\
ML in Automatic Chord Generation \\
SeqGAN \\  
MuseGAN \\      
ChordGAN \\       
CLSTMGAN for melody Generation \\

\section{Model Requirements}
\subsection{MVP Requrements}

The MVP requirements were defined at the beginning of the project to ensure it is compatible with the other parts of the product that were simaltaniously in development and to ensure the possibbility of the creation of the prototye for proof of concept. 
We required the model to be a black box in which we could input a melody and it would output chords which sounded good.
The notion of sounding good is intentionally vague as what sounds good or bad is usually down to individual tast in music.
There is never a definitive answer to which chords would sound the best.
However, we felt \textbf{NEED STRONGER PROOF} that even people without any musical training would be able to judge whether chords fit the melody relatively accurately.
Within this overarching requirement we defined tighter constraints for the sake of both practicality and user experience. 
The model would have to be designed for sequential data such that the temporal relationship of the different notes being played were taken into account.
It would be possible to simply learn a function where for a given measure a chord suited to the notes played within that measure was generated without taking into account surrounding measures.
This approach could produce reasonable results and be significantly more simple that other options, however, the quality and variation of chords generated would be lower than that of a model for sequential data, hence our choice to forego it.
The model would also have to be conditional. For a given input it should produce a catered output. This is an obious contraint however for models such as a GAN it recquires changes to the format of the input data.
To maximise user experience the models should be non-deterministic. This allows users to regenerate the accompaniment multiple times to obtain new chords and thus means they can choose what they feel is best.

One chord per measure
\subsection{Other possible Requrements}

There were some constraints which were not used in our project which would provide better quality chords but provided too large a practical barrier to be implemented.

Strongly temporal
Rhythmic intention conscious
Bidirectional
More than one chord per bar
\section{Models}


\subsection{GANs}

Generative Adversarial Netwoks or GANs were first proposed in the landmark paper \cite{GANs}. Unlike most models GANs consist of two separate models working against each other.
The first model, the Generator, takes an input of random noise and outputs data which mimics the training data. 
The second model, the Discriminator, takes as input an example from the training data or an output from the Generator and outputs a value between 0 and 1 indicating whether it thinks the input is real or generated.
The Binary Cross Entropy loss is used for the Discriminator avaraged across real and generate examples. The loss for the Generator is the log complement of that for the Discriminator. Thus the two models play the following minmax game:
\begin{equation}
\underset{D}{\text{min}} \underset{G}{\text{max}} V(D,G) = \mathbf{E}_{x\sim p_{data}(\mathbf{x})}[logD(\mathbf{x})] + \mathbf{E}_{z\sim p_{data}(\mathbf{z})}[log(1-D(G(\mathbf{z})))]
\end{equation} 
Theoretically the Generator and Discriminator can be any differentaible function thus leaving much room for flexibility.
The problem with GANs for our uses is that they are not conditional, the only input to the Generator is noise. A proposed remody for this was presented in \cite{CGANs}.
By concatinating the label data, in our case the melody, with the noise as input to the Generator and doing the same with the training data and the label data, in our case a chord and melody, as input to the Discriminator GANs can be made conditional. 
Thus they would be suitable for our uses. 
\subsection{RNNs}

Recurrent Neural Networks or RNNs have become a staple in the machine learning engineer's library of models. 
They are structured much like a normal MLP, however, they contain an extra connection to the state of the network in the timestep before.
This means that the state of the network at each timestep depends that of the previous timestep and thus the state at the current timestep is affected by the state in all previous timesteps.
This makes RNNs ideal for processing sequential data as temporal relationships are taken into account.
The gradient of RNNs can be found using a variation of the backpropagation algorithm usually know as backpropagation through time.
RNNs that operate on large sequences often experience the probblems of exploding or vanishing gradients leading to a saturation of learning.

\subsection{LSTMs}

The Long Short Term Memory model or LSTM was proposed in \cite{LSTMs} in order to overcome the gradient problems related to RNNs and thus allow for faster training on long sequences.

\subsection{Transformers}

\subsection{Our Model}

We propose the use of a conditional GAN where the Generator consists of a linear layer followed by a number of LSTM layers and then by another linear layer and the Discriminator consists of a number of LSTM layers followed by a linear output layer.

\section{Training}
Adam optimiser
BCELoss averaged across each element in the sequence or BCELoss for whole song?

\subsection{Issues}

\section{Results}
