% Chapter Template
\tocdata{toc}{All }



\chapter{Conclusion} % Main chapter title
\chaptermark{Conclusion - All}
\label{Conclusion} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\section{Conclusion}

In this work we presented ReChorder, a product designed to be able to provide suitable accompaniment, in the form of chords, to amateur musicians who would not otherwise be able to write their own accompaniment.
We broke the product down into 
product design, specifically: UI and UX; 
a community feature powered by a recommendation engine; 
an audio processor to convert a recording into an useable data format; 
a data processor to extract features from the data and put it in a suitable format for input to the model;
and a machine learning model to generate the chords from an encoded melody. 
\\
From our study of product design, we found out that a good UX through a high-quality UI is the key factor that helps attract and maintain user traffic to an app, further benefiting monetisation. 
Currently, we have only finished the prototype of the design using Figma to identify possible flaws in user flows. 
\\
As an essential part of the community features, we designed a recommendation engine to provide users with the most engaging content.
We found from other work on recommendation engines that a well-designed recommendation system can improve the impression that our users have on our app, further leads a potential increases in sales.
Hence, we designed our product to include both personalised and non-personalised systems, the systems will consider both implicit and explicit information to 
generate recommendations for each user.
\\
To convert raw recorded audio to an useable format (i.e. \emph{note, note\_duration and key}), we found that we needed a noise filter, a pitch detection algorithm and a key detection algorithm. 
Building on the assumptions (see \cref{sec:asm}), we decided to use spectral reduction for the noise filter, the CREPE model for PDA and a template-matching method for KDA.
\\
To train our model, we needed a dataset and a way to encode this data to make it suitable as input. 
We discovered from other works that there exists a training dataset with the necessary audio features extracted from the songs, as well as the steps to pre-process that dataset. 
We then converted the data into formats that are suitable as input to the LSTMGAN as well as the Transformer in order to train both these models. 
During actual operation, the pieces of code used to process the dataset in the training phase can be reused to encode the output of the Voice Processor to feed it into our model. 
The only difference is that the Data Processor's output will no longer contain any chord information.
\\
To generate chords from a melody we needed a suitable model, we explored our options from the literature and determined that a combination of models would likely be an effective solution.
Using this evaluation we proposed the use of an LSTMGAN and detailed its design.
We created a prototype implementation of this model but were unable to properly train and test it due to system difficulties.

\section{Future work}

Future work could explore
\textbf{GENERAL IMPROVEMENTS i.e. relaxations of MVP spec} 
the expansion of the types of accompaniment available, this could come in the form of different chords not limited to just major and minor triads, or percussion.
\\
The user interface is to be updated in a data driven fashion based on the principles of the lean startup loop (\cref{fig:leanloop}). 
A/B tests could be used to determine which configuration of interface users respond best to and iterations of this will lead to a continually improving user experience.
\\
Different approaches to recommender engine design could be sought to improve the quality of recommendations on the community page. 
An optimisation of the balance between the system performance and computational power requirement could be conducted since there is always a trade-off between these two factors.
\\
For the audio pre-processor There are some improvements that can be made in future work. 
For the noise filter, a low-pass filter could be introduced after the spectral reduction to filter out sudden high-frequency noises. 
The collection of data could facilitate the improvement of the performance of the PDA by means of a MAP estimation. 
As for the template-matching KDA, since one limitation is that the profiles that we are using may be out-of-date, the profile could be updated by asking users to rate how well each note fits in a key (replicating the experiment done by Krumhansl and Kessler).
\\
As stated in \cref{Intro to Dataset}, all songs in the training dataset we used are in major key. 
Hence, the use of our model is restricted to songs in major keys; for songs in minor keys, the model will likely perform poorly. 
In future extensions of this project, training datasets with songs in minor key could be included.
The Data Processor may then have to be modified slightly to transpose between minor keys like how it is supposed to transpose between major keys. 
Alternatively, the Data Processor can be coded to transpose between major and minor keys, i.e. from a major key to a minor key and vice versa. 
This will be more complicated than the simple transposition that we had implemented in \cref{preprocessing} \cite{BasicMusicTheoryBook}, but is definitely something worth exploring in future extensions. 
\\
As mentioned in the third pre-processing step in \cref{preprocessing}, more chord types can be easily included in the future as additional independent classes. 
Doing so would allow the model to train to be able to output more varied and complex sequences of chords.
\\
Additionally, we explored the possibility of including a mood classification functionality in \cref{Music Mood Classification}. 
The Audio Pre-processor may have to be modified to extract the necessary extra audio information such as timbre and intensity (\cite{Timbre}), which will be passed to the Data Processor. The Data Processor will then use a Support Vector Machine to regress and cluster the Tellegen-Watson-Clark continuous mood space model using the audio information to estimate the music mood of songs. 
In the training phase, the mood information will be used to train the model to produce chords that are appropriate for specific music moods. 
In the production phase, the model will then be able to output chords for specific moods selected by the user or estimated directly through the user's singing.
\\
There is much room for future work to improve the model.
One direction which could prove fruitful is the relaxation of the MVP requirements. 
Specifically a model could be created which is capable of generating more than one chord per bar.
This would likely require significant additions to the structure of the model as the problem of determining when to play a chord is of similar difficulty to determining which chord to play.
Another possible area for work is to take into account both the order and duration of the notes played within a measure.
This could be achieved by implementing the transformer model detailed in \cref{sec:ModelTransformers}.
An extra benefit of this model is it would be able to lock chords when regenerating thus giving the user more power over the chords generated.
Another improvement to the model would be to add parameters which could be adjusted to change the mood or genre of the generated chords.
Finally, a working implementation of the LSTMGAN would allow the testing of the model for comparison with other options.
A user subjective test could be carried out imitating those in \cite{MySong} and \cite{BLSTM} for best evaluation.
\\

