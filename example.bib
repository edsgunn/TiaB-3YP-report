
@inproceedings{10.1145/2541940.2541967,
  author    = {Chen, Tianshi and Du, Zidong and Sun, Ninghui and Wang, Jia and Wu, Chengyong and Chen, Yunji and Temam, Olivier},
  title     = {DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning},
  year      = {2014},
  isbn      = {9781450323055},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2541940.2541967},
  doi       = {10.1145/2541940.2541967},
  abstract  = {Machine-Learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, a machine-learning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope.Until now, most machine-learning accelerator designs have focused on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy.We show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and neurons outputs additions) in a small footprint of 3.02 mm2 and 485 mW; compared to a 128-bit 2GHz SIMD processor, the accelerator is 117.87x faster, and it can reduce the total energy by 21.08x. The accelerator characteristics are obtained after layout at 65 nm. Such a high throughput in a small footprint can open up the usage of state-of-the-art machine-learning algorithms in a broad set of systems and for a broad set of applications.},
  booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages     = {269-284},
  numpages  = {16},
  keywords  = {memory, neural networks, accelerator},
  location  = {Salt Lake City, Utah, USA},
  series    = {ASPLOS '14}
}
@article{10.1145/2644865.2541967,
  author     = {Chen, Tianshi and Du, Zidong and Sun, Ninghui and Wang, Jia and Wu, Chengyong and Chen, Yunji and Temam, Olivier},
  title      = {DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning},
  year       = {2014},
  issue_date = {April 2014},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {49},
  number     = {4},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/2644865.2541967},
  doi        = {10.1145/2644865.2541967},
  abstract   = {Machine-Learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, a machine-learning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope.Until now, most machine-learning accelerator designs have focused on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy.We show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and neurons outputs additions) in a small footprint of 3.02 mm2 and 485 mW; compared to a 128-bit 2GHz SIMD processor, the accelerator is 117.87x faster, and it can reduce the total energy by 21.08x. The accelerator characteristics are obtained after layout at 65 nm. Such a high throughput in a small footprint can open up the usage of state-of-the-art machine-learning algorithms in a broad set of systems and for a broad set of applications.},
  journal    = {SIGPLAN Not.},
  month      = {feb},
  pages      = {269\-284},
  numpages   = {16},
  keywords   = {memory, neural networks, accelerator}
}

@article{10.1145/2654822.2541967,
  author     = {Chen, Tianshi and Du, Zidong and Sun, Ninghui and Wang, Jia and Wu, Chengyong and Chen, Yunji and Temam, Olivier},
  title      = {DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning},
  year       = {2014},
  issue_date = {March 2014},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {42},
  number     = {1},
  issn       = {0163-5964},
  url        = {https://doi.org/10.1145/2654822.2541967},
  doi        = {10.1145/2654822.2541967},
  abstract   = {Machine-Learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, a machine-learning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope.Until now, most machine-learning accelerator designs have focused on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy.We show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and neurons outputs additions) in a small footprint of 3.02 mm2 and 485 mW; compared to a 128-bit 2GHz SIMD processor, the accelerator is 117.87x faster, and it can reduce the total energy by 21.08x. The accelerator characteristics are obtained after layout at 65 nm. Such a high throughput in a small footprint can open up the usage of state-of-the-art machine-learning algorithms in a broad set of systems and for a broad set of applications.},
  journal    = {SIGARCH Comput. Archit. News},
  month      = {feb},
  pages      = {269\-284},
  numpages   = {16},
  keywords   = {accelerator, neural networks, memory}
} 

@inproceedings{10.1145/2750858.2804262,
  author    = {Lane, Nicholas D. and Georgiev, Petko and Qendro, Lorena},
  title     = {DeepEar: Robust Smartphone Audio Sensing in Unconstrained Acoustic Environments Using Deep Learning},
  year      = {2015},
  isbn      = {9781450335744},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2750858.2804262},
  doi       = {10.1145/2750858.2804262},
  abstract  = {Microphones are remarkably powerful sensors of human behavior and context. However, audio sensing is highly susceptible to wild fluctuations in accuracy when used in diverse acoustic environments (such as, bedrooms, vehicles, or cafes), that users encounter on a daily basis. Towards addressing this challenge, we turn to the field of deep learning; an area of machine learning that has radically changed related audio modeling domains like speech recognition. In this paper, we present DeepEar -- the first mobile audio sensing framework built from coupled Deep Neural Networks (DNNs) that simultaneously perform common audio sensing tasks. We train DeepEar with a large-scale dataset including unlabeled data from 168 place visits. The resulting learned model, involving 2.3M parameters, enables DeepEar to significantly increase inference robustness to background noise beyond conventional approaches present in mobile devices. Finally, we show DeepEar is feasible for smartphones by building a cloud-free DSP-based prototype that runs continuously, using only 6% of the smartphone's battery daily.},
  booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
  pages     = {283\-294},
  numpages  = {12},
  keywords  = {deep learning, audio sensing, mobile sensing},
  location  = {Osaka, Japan},
  series    = {UbiComp '15}
}

 @article{10.1145/3005448,
  author     = {Page, Adam and Jafari, Ali and Shea, Colin and Mohsenin, Tinoosh},
  title      = {SPARCNet: A Hardware Accelerator for Efficient Deployment of Sparse Convolutional Networks},
  year       = {2017},
  issue_date = {July 2017},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {13},
  number     = {3},
  issn       = {1550-4832},
  url        = {https://doi.org/10.1145/3005448},
  doi        = {10.1145/3005448},
  abstract   = {Deep neural networks have been shown to outperform prior state-of-the-art solutions that often relied heavily on hand-engineered feature extraction techniques coupled with simple classification algorithms. In particular, deep convolutional neural networks have been shown to dominate on several popular public benchmarks such as the ImageNet database. Unfortunately, the benefits of deep networks have yet to be fully exploited in embedded, resource-bound settings that have strict power and area budgets. Graphical processing unit (GPU) have been shown to improve throughput and energy-efficiency over central processing unit (CPU) due to their highly parallel architecture yet still impose a significant power burden. In a similar fashion, field programmable gate array (FPGA) can be used to improve performance while further allowing more fine-grained control over implementation to improve efficiency. In order to reduce power and area while still achieving required throughput, classification-efficient network architectures are required in addition to optimal deployment on efficient hardware. In this work, we target both of these enterprises. For the first objective, we analyze simple, biologically inspired reduction strategies that are applied both before and after training. The central theme of the techniques is the introduction of sparsification to help dissolve away the dense connectivity that is often found at different levels in convolutional neural networks. The sparsification techniques include feature compression partition, structured filter pruning, and dynamic feature pruning. Additionally, we explore filter factorization and filter quantization approximation techniques to further reduce the complexity of convolutional layers. In the second contribution, we propose SPARCNet, a hardware accelerator for efficient deployment of SPARse Convolutional NETworks. The accelerator looks to enable deploying networks in such resource-bound settings by both exploiting efficient forms of parallelism inherent in convolutional layers and by exploiting the sparsification and approximation techniques proposed. To demonstrate both contributions, modern deep convolutional network architectures containing millions of parameters are explored within the context of the computer vision dataset CIFAR. Utilizing the reduction techniques, we demonstrate the ability to reduce computation and memory by 60% and 93% with less than 0.03% impact on accuracy when compared to the best baseline network with 93.47% accuracy. The SPARCNet accelerator with different numbers of processing engines is implemented on a low-power Artix-7 FPGA platform. Additionally, the same networks are optimally implemented on a number of embedded commercial-off-the-shelf platforms including NVIDIAs CPU+GPU SoCs TK1 and TX1 and Intel Edison. Compared to NVIDIAs TK1 and TX1, the FPGA-based accelerator obtains 11.8 \texttimes{} and 7.5 \texttimes{} improvement in energy efficiency while maintaining a classification throughput of 72 images/s. When further compared to a number of recent FPGA-based accelerators, SPARCNet is able to achieve up to 15 \texttimes{} improvement in energy efficiency while consuming less than 2W of total board power at 100MHz. In addition to improving efficiency, the accelerator has built-in support for sparsification techniques and ability to perform in-place rectified linear unit (ReLU) activation function, max-pooling, and batch normalization.},
  journal    = {J. Emerg. Technol. Comput. Syst.},
  month      = {may},
  articleno  = {31},
  numpages   = {32},
  keywords   = {Deep learning, embedded applications, reduction techniques}
} 

 @inproceedings{10.1145/3210240.3210337,
  author    = {Liu, Sicong and Lin, Yingyan and Zhou, Zimu and Nan, Kaiming and Liu, Hui and Du, Junzhao},
  title     = {On-Demand Deep Model Compression for Mobile Devices: A Usage-Driven Model Selection Framework},
  year      = {2018},
  isbn      = {9781450357203},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3210240.3210337},
  doi       = {10.1145/3210240.3210337},
  abstract  = {Recent research has demonstrated the potential of deploying deep neural networks (DNNs) on resource-constrained mobile platforms by trimming down the network complexity using different compression techniques. The current practice only investigate stand-alone compression schemes even though each compression technique may be well suited only for certain types of DNN layers. Also, these compression techniques are optimized merely for the inference accuracy of DNNs, without explicitly considering other application-driven system performance (e.g. latency and energy cost) and the varying resource availabilities across platforms (e.g. storage and processing capability). In this paper, we explore the desirable tradeoff between performance and resource constraints by user-specified needs, from a holistic system-level viewpoint. Specifically, we develop a usage-driven selection framework, referred to as AdaDeep, to automatically select a combination of compression techniques for a given DNN, that will lead to an optimal balance between user-specified performance goals and resource constraints. With an extensive evaluation on five public datasets and across twelve mobile devices, experimental results show that AdaDeep enables up to 9.8x latency reduction, 4.3x energy efficiency improvement, and 38x storage reduction in DNNs while incurring negligible accuracy loss. AdaDeep also uncovers multiple effective combinations of compression techniques unexplored in existing literature.},
  booktitle = {Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services},
  pages     = {389\-400},
  numpages  = {12},
  keywords  = {deep learning, model compression, deep reinforcement learning},
  location  = {Munich, Germany},
  series    = {MobiSys '18}
} 

 @inproceedings{10.1145/3241539.3241563,
  author    = {Xu, Mengwei and Zhu, Mengze and Liu, Yunxin and Lin, Felix Xiaozhu and Liu, Xuanzhe},
  title     = {DeepCache: Principled Cache for Mobile Deep Vision},
  year      = {2018},
  isbn      = {9781450359030},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3241539.3241563},
  doi       = {10.1145/3241539.3241563},
  abstract  = {We present DeepCache, a principled cache design for deep learning inference in continuous mobile vision. DeepCache benefits model execution efficiency by exploiting temporal locality in input video streams. It addresses a key challenge raised by mobile vision: the cache must operate under video scene variation, while trading off among cacheability, overhead, and loss in model accuracy. At the input of a model, DeepCache discovers video temporal locality by exploiting the video's internal structure, for which it borrows proven heuristics from video compression; into the model, DeepCache propagates regions of reusable results by exploiting the model's internal structure. Notably, DeepCache eschews applying video heuristics to model internals which are not pixels but high-dimensional, difficult-to-interpret data. Our implementation of DeepCache works with unmodified deep learning models, requires zero developer's manual effort, and is therefore immediately deployable on off-the-shelf mobile devices. Our experiments show that DeepCache saves inference execution time by 18% on average and up to 47%. DeepCache reduces system energy consumption by 20% on average.},
  booktitle = {Proceedings of the 24th Annual International Conference on Mobile Computing and Networking},
  pages     = {129\-144},
  numpages  = {16},
  keywords  = {mobile vision, deep learning, cache},
  location  = {New Delhi, India},
  series    = {MobiCom '18}
} 

 @inproceedings{7460664,
  author    = {Lane, Nicholas D. and Bhattacharya, Sourav and Georgiev, Petko and Forlivesi, Claudio and Jiao, Lei and Qendro, Lorena and Kawsar, Fahim},
  booktitle = {2016 15th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)},
  title     = {DeepX: A Software Accelerator for Low-Power Deep Learning Inference on Mobile Devices},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {1-12},
  doi       = {10.1109/IPSN.2016.7460664}
} 

 @misc{abrsm2014,
  title     = {Teaching, learning and playing in the UK},
  url       = {https://gb.abrsm.org/media/12032/makingmusic2014.pdf},
  journal   = {ABRSM Making Music Report 2014},
  publisher = {ABRSM},
  year      = {2014},
  month     = {Sep}
} 

 @misc{abrsm2021,
  title     = {Learning, playing and teaching in the UK in 2021},
  url       = {https://gb.abrsm.org/media/66373/web_abrsm-making-music-uk-21.pdf},
  journal   = {ABRSM Making Music Report 2021},
  publisher = {ABRSM},
  year      = {2021}
} 

@misc{Adam,
  doi       = {10.48550/ARXIV.1412.6980},
  url       = {https://arxiv.org/abs/1412.6980},
  author    = {Kingma, Diederik P. and Ba, Jimmy},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Adam: A Method for Stochastic Optimization},
  publisher = {arXiv},
  year      = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

 @misc{allmusic,
  title     = {Explore music by mood},
  url       = {https://www.allmusic.com/moods},
  journal   = {AllMusic},
  publisher = {AllMusic}
} 

 @misc{applecoreml,
  title     = {Core ML},
  url       = {https://developer.apple.com/documentation/coreml},
  journal   = {Apple Developer Documentation},
  publisher = {Apple Inc.}
} 

 @misc{audacity,
  title   = {How audacity noise reduction works},
  url     = {https://wiki.audacityteam.org/wiki/How_Audacity_Noise_Reduction_Works#:~:text=The%20noise%20reduction%20algorithm%20uses,noise%20in%20your%20sound%20file.},
  journal = {How Audacity Noise Reduction Works - Audacity Wiki}
} 

 @misc{aws,
  title   = {Sustainability in the cloud},
  url     = {https://sustainability.aboutamazon.com/environment/the-cloud?energyType=true},
  journal = {Sustainability}
} 

@inproceedings{Bertin-Mahieux2011,
  author    = {Thierry Bertin-Mahieux and Daniel P.W. Ellis and Brian Whitman and Paul Lamere},
  title     = {The Million Song Dataset},
  booktitle = {{Proceedings of the 12th International Conference on Music Information
               Retrieval ({ISMIR} 2011)}},
  year      = {2011},
  owner     = {thierry},
  timestamp = {2010.03.07}
} 

@misc{bessel,
  title   = {Bessel filter},
  url     = {https://www.sciencedirect.com/topics/engineering/bessel-filter},
  journal = {Bessel Filter - an overview | ScienceDirect Topics}
}

@misc{BLSTM,
  doi       = {10.48550/ARXIV.1712.01011},
  url       = {https://arxiv.org/abs/1712.01011},
  author    = {Lim, Hyungui and Rhyu, Seungyeon and Lee, Kyogu},
  keywords  = {Sound (cs.SD), Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title     = {Chord Generation from Symbolic Melody Using BLSTM Networks},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{bookreview,
  issn            = {00109894},
  url             = {http://www.jstor.org/stable/40319425},
  author          = {Robert H. Woody},
  journal         = {Bulletin of the Council for Research in Music Education},
  number          = {155},
  pages           = {81--85},
  publisher       = {University of Illinois Press},
  reviewed-author = {Lucy Green},
  urldate         = {2022-04-14},
  year            = {2003}
}

 @misc{brown2018,
  title     = {Royal College of Music Head criticises decline in provision in schools},
  url       = {https://www.theguardian.com/education/2018/mar/14/royal-college-of-music-head-criticises-state-of-school-music-provision-budget-cuts},
  journal   = {The Guardian},
  publisher = {Guardian News and Media},
  author    = {Brown, Mark},
  year      = {2018},
  month     = {Mar}
} 

 @article{carbon,
  url     = {https://www.computerweekly.com/feature/Datacentre-energy-efficiency-Is-the-time-now-for-a-big-switch-off},
  journal = {Datacentre energy efficiency: Is the time now for a big switch-off?},
  author  = {Doidge, Fleur},
  year    = {2021},
  month   = {Jan}
} 

 @misc{CGANs,
  doi       = {10.48550/ARXIV.1411.1784},
  url       = {https://arxiv.org/abs/1411.1784},
  author    = {Mirza, Mehdi and Osindero, Simon},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Conditional Generative Adversarial Nets},
  publisher = {arXiv},
  year      = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
} 

 @inproceedings{Chen2015MachineLI,
  title  = {Machine Learning in Automatic Music Chords Generation},
  author = {Ziheng Chen and Jie Qi},
  year   = {2015}
} 

 @inproceedings{ChordGAN,
  title  = {ChordGAN: Symbolic Music Style Transfer with Chroma Feature Extraction},
  author = {Conan Lu and Shlomo Dubnov},
  year   = {2021}
} 

@article{ChordPrediction,
  title     = {An intelligent hybrid model for chord prediction},
  volume    = {4},
  doi       = {10.1017/S1355771899002071},
  number    = {2},
  journal   = {Organised Sound},
  publisher = {Cambridge University Press},
  author    = {CUNHA, URAQUITAN SIDNEY and RAMALHO, GEBER},
  year      = {1999},
  pages     = {115-119}
}


@misc{cisco_2022,
  title     = {Cisco annual internet Report - Cisco Annual Internet Report (2018-2023) White Paper},
  url       = {https://www.cisco.com/c/en/us/solutions/collateral/executive-perspectives/annual-internet-report/white-paper-c11-741490.html},
  journal   = {Cisco},
  publisher = {Cisco},
  year      = {2022},
  month     = {Jan}
}

 @book{cloud,
  url         = {https://www.nrdc.org/sites/default/files/cloud-computing-efficiency-IB.pdf},
  journal     = {Is Cloud Computing Always Greener?},
  institution = {Natural Resources Defense Council},
  year        = {2012},
  month       = {Oct}
} 

 @book{cloudpercent,
  url         = {https://crd.lbl.gov/assets/pubs_presos/ACS/cloud_efficiency_study.pdf},
  journal     = {The Energy Efficiency Potential of Cloud-Based Software: A U.S. Case Study},
  institution = {Google},
  author      = {Masanet, Eric and Liang, Jiaqi and Ma, Xiaohui and Walker, Benjamin and Shehabi, Arman and Ramakrishnan, Lavanya and Hendrix, Valerie and Mantha, Pradeep},
  year        = {2013},
  month       = {Jun}
} 

 @misc{codeofethics,
  title     = {Code of ethics},
  url       = {https://www.nspe.org/resources/ethics/code-ethics#:~:text=Engineering%20has%20a%20direct%20and,health%2C%20safety%2C%20and%20welfare.},
  journal   = {Code of Ethics | National Society of Professional Engineers},
  publisher = {National Society of Professional Engineers}
} 

 @article{compareguitarpiano,
  title   = {Music motivation and the effect of writing music: A comparison of pianists and guitarists},
  author  = {Peter D. MacIntyre and Gillian K. Potter},
  journal = {Psychology of Music},
  year    = {2014},
  volume  = {42},
  number  = {3},
  pages   = {403-419},
  doi     = {10.1177/0305735613477180},
  eprint  = {https://doi.org/10.1177/0305735613477180},
  url     = {https://doi.org/10.1177/0305735613477180}
} 

 @phdthesis{complexwt,
  title  = {Complex Wavelet Transform},
  url    = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.102.6504&amp;rep=rep1&amp;type=pdf},
  author = {D Shukla, Panchamkumar},
  year   = {2003}
} 

@misc{counterpointML,
  title     = {The rise of AI: One in three smartphones will be AI capable in 2020},
  url       = {https://www.counterpointresearch.com/the-rise-of-ai-one-in-three-smartphones-will-be-ai-capable-in-2020/},
  journal   = {Counterpoint Research},
  publisher = {Counterpoint},
  author    = {Srivastava, Shobhit},
  year      = {2017},
  month     = {Oct}
}

 @misc{cybersecurityventures,
  title     = {Humans on the internet will triple from 2015 to 2022 and hit 6 Billion},
  url       = {https://cybersecurityventures.com/how-many-internet-users-will-the-world-have-in-2022-and-in-2030/},
  journal   = {Cybercrime Magazine},
  publisher = {Cybersecurity Ventures},
  author    = {Sausalito, Calif.},
  year      = {2019},
  month     = {Jul}
} 

 @misc{DeloitteData,
  title     = {Data Privacy: Digital Consumer trends},
  url       = {https://www2.deloitte.com/uk/en/pages/technology-media-and-telecommunications/articles/digital-consumer-trends-data-privacy.html},
  journal   = {Deloitte United Kingdom},
  publisher = {Deloitte},
  author    = {Lee, Paul and Calugar-Pop, Cornelia},
  year      = {2020},
  month     = {Oct}
} 


 @inproceedings{DL3,
  author = {Kang, Yiping and Hauswald, Johann and Gao, Cao and Rovinski, Austin and Mudge, Trevor and Mars, Jason and Tang, Lingjia},
  year   = {2017},
  month  = {04},
  pages  = {615-629},
  title  = {Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge},
  doi    = {10.1145/3037697.3037698}
} 

 @misc{energy,
  title   = {We need to grow green - the global energy consumption of Information Technologies [infographic]},
  url     = {https://www.go-globe.com/we-need-to-grow-green-the-global-energy-consumption-of-information-technologies-infographic/},
  journal = {GO},
  year    = {2021},
  month   = {Nov}
} 

@inproceedings{energyplanguage,
  author     = {Pereira, Rui and Couto, Marco and Ribeiro, Francisco and Rua, Rui and Cunha, J{\'a}come and Fernandes, Jo{\~a}o and Saraiva, Jo{\~a}o},
  doi        = {10.1145/3136014.3136031},
  month      = {10},
  pages      = {256-267},
  title      = {Energy efficiency across programming languages: how do energy, time, and memory relate?},
  year       = {2017},
  bdsk-url-1 = {https://doi.org/10.1145/3136014.3136031}
}

@article{energypy,
  url       = {https://science.time.com/2013/08/14/power-drain-the-digital-cloud-is-using-more-energy-than-you-think/},
  journal   = {The Surprisingly Large Energy Footprint of the Digital Economy},
  publisher = {Time},
  author    = {Walsh, Bryan},
  year      = {2013},
  month     = {Aug}
}

@article{epsilon,
  url   = {https://www.epsilon.com/us/about-us/pressroom/new-epsilon-research-indicates-80-of-consumers-are-more-likely-to-make-a-purchase-when-brands-offer-personalized-experiences},
  year  = {2018},
  month = {Jan}
}

@online{EUdataregulations2018,
  title        = {2018 reform of EU data protection rules},
  url          = {https://ec.europa.eu/commission/sites/beta-political/files/data-protection-factsheet-changes_en.pdf},
  organization = {European Commission},
  date         = {2018-05-25},
  urldate      = {2019-06-17}
} 

 @article{f0age,
  author     = {Stathopoulos, Elaine and Huber, Jessica and Sussman, Joan},
  doi        = {10.1044/1092-4388(2010/10-0036)},
  journal    = {Journal of speech, language, and hearing research : JSLHR},
  month      = {08},
  pages      = {1011-21},
  title      = {Changes in Acoustic Characteristics of the Voice Across the Life Span: Measures From Individuals 4-93 Years of Age},
  volume     = {54},
  year       = {2011},
  bdsk-url-1 = {https://doi.org/10.1044/1092-4388(2010/10-0036)}
} 

@misc{framework,
  title     = {Data Ethics Framework},
  url       = {https://www.gov.uk/government/publications/data-ethics-framework},
  journal   = {GOV.UK},
  publisher = {GOV.UK},
  author    = {Office, Central Digital and Data},
  year      = {2020},
  month     = {Sep}
} 

@article{francis,
  url     = {https://www.kentonline.co.uk/kent/news/new-law-will-curb-online-abuse-and-harassment-259155/},
  journal = {Online Safety Bill: The government is promising tougher rules on abuse and harassment},
  author  = {Francis, Paul},
  year    = {2021},
  month   = {Dec}
} 
	
@misc{gallup2003,
  title     = {Gallup Organization reveals findings of "American attitudes toward making music" survey},
  url       = {https://www.namm.org/news/press-releases/gallup-organization-reveals-findings-american-atti},
  journal   = {NAMM.org},
  publisher = {National Association of Music Merchants},
  author    = {Carlsbad, Calif},
  year      = {2003},
  month     = {Apr}
}

@inproceedings{GANs,
  author     = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle  = {Advances in Neural Information Processing Systems},
  editor     = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
  publisher  = {Curran Associates, Inc.},
  title      = {Generative Adversarial Nets},
  url        = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
  volume     = {27},
  year       = {2014},
  bdsk-url-1 = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf}
}

@misc{GlobalSurvey,
  url       = {https://www.globalsurvey-sdgs.com/wp-content/uploads/2020/01/20200205_SC_Global_Survey_Result-Report_english_final.pdf},
  publisher = {The Global Survey on Sustainability and the SDGs}
}

@article{HMM1,
  author     = {Leonard E. Baum and Ted Petrie},
  doi        = {10.1214/aoms/1177699147},
  journal    = {The Annals of Mathematical Statistics},
  number     = {6},
  pages      = {1554 -- 1563},
  publisher  = {Institute of Mathematical Statistics},
  title      = {{Statistical Inference for Probabilistic Functions of Finite State Markov Chains}},
  url        = {https://doi.org/10.1214/aoms/1177699147},
  volume     = {37},
  year       = {1966},
  bdsk-url-1 = {https://doi.org/10.1214/aoms/1177699147}
}


@article{HMM2,
  author     = {Leonard E. Baum and J. A. Eagon},
  doi        = {bams/1183528841},
  journal    = {Bulletin of the American Mathematical Society},
  number     = {3},
  pages      = {360 -- 363},
  publisher  = {American Mathematical Society},
  title      = {{An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model for ecology}},
  url        = {https://doi.org/},
  volume     = {73},
  year       = {1967},
  bdsk-url-1 = {https://doi.org/},
  bdsk-url-2 = {https://doi.org/bams/1183528841}
}


@article{HMM3,
  author     = {Leonard E. Baum and Ted Petrie and George Soules and Norman Weiss},
  doi        = {10.1214/aoms/1177697196},
  journal    = {The Annals of Mathematical Statistics},
  number     = {1},
  pages      = {164 -- 171},
  publisher  = {Institute of Mathematical Statistics},
  title      = {{A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains}},
  url        = {https://doi.org/10.1214/aoms/1177697196},
  volume     = {41},
  year       = {1970},
  bdsk-url-1 = {https://doi.org/10.1214/aoms/1177697196}
}

@article{HMMswithML,
  abstract      = {Accompaniment production is one of the most important elements in music work, and chord arrangement is the key link of accompaniment production, which usually requires more musical talent and profound music theory knowledge to be competent. In this article, the machine learning model is used to replace manual accompaniment chords\&{\#}x2019; arrangement, and an automatic computer means is provided to complete and assist accompaniment chords\&{\#}x2019; arrangement. Also, through music feature extraction, automatic chord label construction, and model construction and training, the whole system finally has the ability of automatic accompaniment chord arrangement for the main melody. Based on the research of automatic chord label construction method and the characteristics of MIDI data format, a chord analysis method based on interval difference is proposed to construct chord labels of the whole track and realize the construction of automatic chord labels. In this study, the hidden Markov model is constructed according to the chord types, in which the input features are the improved theme PCP features proposed in this paper, and the input labels are the label data set constructed by the automated method proposed in this paper. After the training is completed, the PCP features of the theme to be predicted and improved are input to generate the accompaniment chords of the final arrangement. Through PCP features and template-matching model, the system designed in this paper improves the matching accuracy of the generated chords compared with that generated by the traditional method.},
  author        = {Chai, Chunlai and Shi, Shuo and Xi, Shuting and Tsai, Sang-Bing},
  da            = {2021/10/14},
  date-added    = {2022-04-04 19:50:43 +0100},
  date-modified = {2022-04-04 19:50:43 +0100},
  doi           = {10.1155/2021/6551493},
  isbn          = {1024-123X},
  journal       = {Mathematical Problems in Engineering},
  pages         = {6551493},
  publisher     = {Hindawi},
  title         = {Research on Autoarrangement System of Accompaniment Chords Based on Hidden Markov Model with Machine Learning},
  ty            = {JOUR},
  url           = {https://doi.org/10.1155/2021/6551493},
  volume        = {2021},
  year          = {2021},
  bdsk-url-1    = {https://doi.org/10.1155/2021/6551493}
}

 @incollection{HPS,
  title  = {Harmonic product spectrum revisited and adapted for rotating machine monitoring based on IAS},
  author = {Andre, QL Hugo and Khelf, Ilyes and Leclere, Quentin}
} 

@misc{https://doi.org/10.48550/arxiv.1704.04861,
  doi       = {10.48550/ARXIV.1704.04861},
  url       = {https://arxiv.org/abs/1704.04861},
  author    = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{humanmono,
  title     = {Harmonic structure},
  url       = {https://www.britannica.com/topic/speech-language/Harmonic-structure},
  journal   = {Encyclopædia Britannica},
  publisher = {Encyclopædia Britannica, inc.}
}


@article{humanrange,
  title   = {Preferences for very low and very high voice pitch in humans},
  volume  = {7},
  doi     = {10.1371/journal.pone.0032719},
  number  = {3},
  journal = {PLoS ONE},
  author  = {Re, Daniel E. and O'Connor, Jillian J. and Bennett, Patrick J. and Feinberg, David R.},
  year    = {2012}
}

@misc{ibrahem,
  title  = {Image Processing},
  url    = {https://uotechnology.edu.iq/ce/lecture%202013n/4th%20Image%20Processing%20_Lectures/DIP_Lecture9.pdf},
  author = {Ibrahem, Wassem Nahy}
}

@misc{independentartists,
  title     = {Why independent musicians are becoming the future of the music industry},
  url       = {https://www.forbes.com/sites/melissamdaniels/2019/07/10/for-independent-musicians-goingyour-own-way-is-finally-starting-to-pay-off/?sh=27b388ed14f2},
  journal   = {Forbes},
  publisher = {Forbes Magazine},
  author    = {Daniels, Melissa},
  year      = {2019},
  month     = {Jul}
}

 @misc{iphoneaud,
  title   = {Audio Session Programming Guide},
  url     = {https://developer.apple.com/library/archive/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/OptimizingForDeviceHardware/OptimizingForDeviceHardware.html},
  journal = {Configuring Device Hardware},
  year    = {2017},
  month   = {Sep}
} 

 @book{leanstartuploop,
  abstract    = {Most startups are built to fail. But those failures, according to entrepreneur Eric Ries, are preventable. Startups don't fail because of bad execution, or missed deadlines, or blown budgets. They fail because they are building something nobody wants. Whether they arise from someone's garage or are created within a mature Fortune 500 organization, new ventures, by definition, are designed to create new products or services under conditions of extreme uncertainly. Their primary mission is to find out what customers ultimately will buy. One of the central premises of The Lean Startup movement is what Ries calls "validated learning" about the customer. It is a way of getting continuous feedback from customers so that the company can shift directions or alter its plans inch by inch, minute by minute. Rather than creating an elaborate business plan and a product-centric approach, Lean Startup prizes testing your vision continuously with your customers and making constant adjustments.},
  added-at    = {2014-03-30T19:18:35.000+0200},
  address     = {London; New York},
  author      = {Ries, Eric},
  biburl      = {https://www.bibsonomy.org/bibtex/212c0c1b86e809855249bdf016acb6a9e/tobi3112},
  description = {The Lean Startup: How Constant Innovation Creates Radically Successful Businesses: How Relentless Change Creates Radically Successful Businesses: Amazon.de: Eric Ries: Englische Bücher},
  interhash   = {ca76a5ba307e415da1550b36d8660946},
  intrahash   = {12c0c1b86e809855249bdf016acb6a9e},
  isbn        = {9780670921607 0670921602},
  keywords    = {ba},
  publisher   = {Portfolio Penguin},
  refid       = {769815516},
  timestamp   = {2014-03-30T19:18:35.000+0200},
  title       = {The lean startup : how constant innovation creates radically successful businesses},
  url         = {http://www.amazon.de/The-Lean-Startup-Innovation-Successful/dp/0670921602/ref=sr_1_2?ie=UTF8&qid=1396199893&sr=8-2&keywords=eric+ries},
  year        = {2011}
} 

  @misc{lowtole,
  title     = {Users have low tolerance for buggy apps - only 16% will try a failing app more than twice},
  url       = {https://techcrunch.com/2013/03/12/users-have-low-tolerance-for-buggy-apps-only-16-will-try-a-failing-app-more-than-twice/?guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAMXWe9oo4bg2Jt_j4X7Y6lkc0ov__e-kp3lR30RBa9gNp00C5W_V7oU4XHOoqFFEe9GzkwvId0o1gMi5jO7iV65SYqL5WBtXtgbGsvpF0rofC08YNRCFF_SaXjXQWFwVozt5pKzsuawYPwm4alhRXJP8OvIKFQXDkYazf9P5wAS3&amp;_guc_consent_skip=1650905533},
  journal   = {TechCrunch},
  publisher = {TechCrunch},
  author    = {Perez, Sarah},
  year      = {2013},
  month     = {Mar}
} 

  @article{LSTMs,
  author  = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  year    = {1997},
  month   = {12},
  pages   = {1735-80},
  title   = {Long Short-term Memory},
  volume  = {9},
  journal = {Neural computation},
  doi     = {10.1162/neco.1997.9.8.1735}
}

  @misc{macdonald2021,
  title     = {'Act of vandalism': Outrage as English government cuts to arts and Music Education approved},
  url       = {https://www.classicfm.com/music-news/outrage-english-government-cuts-to-arts-music/},
  journal   = {Classic FM},
  publisher = {Classic FM},
  author    = {Macdonald, Kyle},
  year      = {2021},
  month     = {Jul}
}

@article{marketresearch5,
  title     = {The self-taught career musician: Investigating learning sources and experiences.},
  author    = {Watson, Leah},
  journal   = {Victorian Journal of Music Education},
  volume    = {1},
  pages     = {3--8},
  year      = {2016},
  publisher = {ERIC}
}

@article{medleydb,
  title  = {“Medleydb: A multitrack dataset for annotation-intensive mir research.,”},
  url    = {https://medleydb.weebly.com/},
  author = {Bittner, Rachel M and Salamon, Justin and Tierney, Mike and Mauch, Matthias and Cannam, Chris and Bello, Juan Pablo}
} 

@inproceedings{MLForChords,
  title  = {Machine Learning in Automatic Music Chords Generation},
  author = {Ziheng Chen and Jie Qi},
  year   = {2015}
} 

@inproceedings{MySong,
  author    = {Simon, Ian and Morris, Dan and Basu, Sumit},
  title     = {MySong: Automatic Accompaniment Generation for Vocal Melodies},
  year      = {2008},
  isbn      = {9781605580111},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1357054.1357169},
  doi       = {10.1145/1357054.1357169},
  abstract  = {We introduce MySong, a system that automatically chooses chords to accompany a vocal melody. A user with no musical experience can create a song with instrumental accompaniment just by singing into a microphone, and can experiment with different styles and chord patterns using interactions designed to be intuitive to non-musicians.We describe the implementation of MySong, which trains a Hidden Markov Model using a music database and uses that model to select chords for new melodies. Model parameters are intuitively exposed to the user. We present results from a study demonstrating that chords assigned to melodies using MySong and chords assigned manually by musicians receive similar subjective ratings. We then present results from a second study showing that thirteen users with no background in music theory are able to rapidly create musical accompaniments using MySong, and that these accompaniments are rated positively by evaluators.},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  pages     = {725-734},
  numpages  = {10},
  keywords  = {hidden markov models, music},
  location  = {Florence, Italy},
  series    = {CHI '08}
} 

@misc{oinkina,
  year    = {2015},
  month   = {8},
  title   = {Understanding LSTM networks},
  url     = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/},
  journal = {Understanding LSTM Networks -- colah's blog},
  author  = {Oinkina}
} 

@article{parliamentlaw,
  url       = {https://www.gov.uk/government/news/world-first-online-safety-laws-introduced-in-parliament},
  journal   = {World-first online safety laws introduced in Parliament},
  publisher = {Department for Digital, Culture, Media &amp; Sport and The Rt Hon Nadine Dorries MP},
  year      = {2022},
  month     = {Mar}
} 

@misc{perilousstate,
  title     = {The State of Play: A Review of Music Education in England},
  publisher = {Musicians' Union},
  author    = {Savage, Jonathan and Barnard, David},
  year      = {2019},
  month     = {Jan}
} 

@misc{praat,
  url     = {https://www.fon.hum.uva.nl/praat/manual/Intro_4_2__Configuring_the_pitch_contour.html},
  journal = {Measuring F0 Contours using Praat},
  author  = {Goldstein, Louis}
}

@misc{principles,
  title   = {Data ethics principles },
  url     = {https://dataethics.eu/data-ethics-principles/},
  journal = {Dataetisk Tænkehandletank},
  author  = {Author:Dataethics},
  year    = {2021},
  month   = {Feb}
}  

@misc{reductionmanual,
  title     = {Noise Reduction},
  url       = {https://sound.eti.pg.gda.pl/denoise/noise.html#:~:text=The%20spectral%20subtraction%20method%20is,ratio%20(SNR)%20is%20improved.},
  journal   = {Multitask Noisy Speech Enhancement System},
  publisher = { Multimedia Systems Department, Gdansk University of Technology and Air Force Academy in Deblin},
  year      = {2004}
} 

@article{ReinforcementLearning,
  abstract       = {We present a novel reinforcement learning architecture that learns a structured representation for use in symbolic melody harmonization. Probabilistic models are predominant in melody harmonization tasks, most of which only treat melody notes as independent observations and do not take note of substructures in the melodic sequence. To fill this gap, we add substructure discovery as a crucial step in automatic chord generation. The proposed method consists of a structured representation module that generates hierarchical structures for the symbolic melodies, a policy module that learns to break a melody into segments (whose boundaries concur with chord changes) and phrases (the subunits in segments) and a harmonization module that generates chord sequences for each segment. We formulate the structure discovery process as a sequential decision problem with a policy gradient RL method selecting the boundary of each segment or phrase to obtain an optimized structure. We conduct experiments on our preprocessed HookTheory Lead Sheet Dataset, which has 17,979 melody/chord pairs. The results demonstrate that our proposed method can learn task-specific representations and, thus, yield competitive results compared with state-of-the-art baselines.},
  article-number = {2469},
  author         = {Zeng, Te and Lau, Francis C. M.},
  doi            = {10.3390/electronics10202469},
  issn           = {2079-9292},
  journal        = {Electronics},
  number         = {20},
  title          = {Automatic Melody Harmonization via Reinforcement Learning by Exploring Structured Representations for Melody Sequences},
  url            = {https://www.mdpi.com/2079-9292/10/20/2469},
  volume         = {10},
  year           = {2021},
  bdsk-url-1     = {https://www.mdpi.com/2079-9292/10/20/2469},
  bdsk-url-2     = {https://doi.org/10.3390/electronics10202469}
}

@misc{rollingstone,
  title     = {A new report says independent artists could generate more than 2 billion in 2020},
  url       = {https://www.rollingstone.com/pro/features/raine-group-independent-artists-2-billion-in-2020-967138/},
  journal   = {Rolling Stone},
  publisher = {Rolling Stone},
  author    = {Ingham, Tim},
  year      = {2020},
  month     = {Mar}
}

@article{rwcdb,
  url    = {https://staff.aist.go.jp/m.goto/RWC-MDB/},
  author = {Goto, Masataka and Oka, Ryuichi and Nishimura, Takuichi and Hashiguchi, Hiroki},
  year   = {2002},
  month  = {Oct}
}

@misc{selfnoise,
  title  = {What is microphone self-noise? (equivalent noise level ...)},
  url    = {https://mynewmicrophone.com/self-noise/},
  author = {Arthur Fox}
}

@article{spectral,
  title     = {Finding, visualizing, and quantifying latent structure across diverse animal vocal repertoires},
  author    = {Sainburg, Tim and Thielk, Marvin and Gentner, Timothy Q},
  journal   = {PLoS computational biology},
  volume    = {16},
  number    = {10},
  pages     = {e1008228},
  year      = {2020},
  publisher = {Public Library of Science}
}

@phdthesis{spectral_drawback,
  title  = {Implementation and Evaluation of Spectral Subtraction with Minimum Statistics using WOLA and FFT Modulated Filter Banks },
  url    = {https://www.diva-portal.org/smash/get/diva2:830201/FULLTEXT01.pdf},
  author = {Rao, Peddi Srinivas and Sreelatha, Vallabhaneni},
  ear    = {2014},
  month  = {Jan}
}

@software{spectralcode,
  author    = {Tim Sainburg},
  title     = {timsainb/noisereduce: v1.0},
  month     = {jun},
  year      = {2019},
  publisher = {Zenodo},
  version   = {db94fe2},
  doi       = {10.5281/zenodo.3243139},
  url       = {https://doi.org/10.5281/zenodo.3243139}
}

@article{spectralflowchart,
  title   = {Noise removal in speech processing using spectral subtraction},
  volume  = {05},
  doi     = {10.4236/jsip.2014.52006},
  number  = {02},
  journal = {Journal of Signal and Information Processing},
  author  = {Karam, Marc and Khazaal, Hasan F. and Aglan, Heshmat and Cole, Cliston},
  year    = {2014},
  pages   = {32-41}
}

@inproceedings{techstratdefinition,
  title  = {Aligning Enterprise, System, and Software Architectures},
  author = {Ivan Mistr{\'i}k and Antony Tang and Rami Bahsoon and Judith A. Stafford},
  year   = {2012}
}

@book{template,
  place     = {New York},
  title     = {Cognitive Foundations of musical pitch},
  publisher = {Oxford University Press},
  author    = {Krumhansl, Carol L.},
  year      = {2001}
}

@article{templatedata,
  title   = {Tracing the dynamic changes in perceived tonal organization in a spatial representation of musical keys.},
  volume  = {89},
  doi     = {10.1037/0033-295x.89.4.334},
  number  = {4},
  journal = {Psychological Review},
  author  = {Krumhansl, Carol L. and Kessler, Edward J.},
  year    = {1982},
  pages   = {334-368}
} 

@misc{tencent,
  title     = {Tencent/NCNN: NCNN is a high-performance neural network inference framework optimized for the Mobile Platform},
  url       = {https://github.com/Tencent/ncnn},
  journal   = {GitHub},
  publisher = {Tencent},
  author    = {Tencent}
} 

@misc{tensorflow,
  title     = {Tensorflow Lite: ML for Mobile and edge devices},
  url       = {https://www.tensorflow.org/lite},
  journal   = {TensorFlow},
  publisher = {Google}
} 

@article{Transformers,
  author     = {Ashish Vaswani and
                Noam Shazeer and
                Niki Parmar and
                Jakob Uszkoreit and
                Llion Jones and
                Aidan N. Gomez and
                Lukasz Kaiser and
                Illia Polosukhin},
  title      = {Attention Is All You Need},
  journal    = {CoRR},
  volume     = {abs/1706.03762},
  year       = {2017},
  url        = {http://arxiv.org/abs/1706.03762},
  eprinttype = {arXiv},
  eprint     = {1706.03762},
  timestamp  = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

 @misc{ukri,
  title   = {Framework for Responsible Innovation},
  url     = {https://www.ukri.org/about-us/epsrc/our-policies-and-standards/framework-for-responsible-innovation/},
  journal = {UKRI},
  year    = {2022},
  month   = {Mar}
} 

 @misc{Wasserstein,
  doi       = {10.48550/ARXIV.1701.07875},
  url       = {https://arxiv.org/abs/1701.07875},
  author    = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
  keywords  = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Wasserstein GAN},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
} 

 @misc{wavelet,
  title   = {Fourier vs. wavelet transform: What's the difference?},
  url     = {https://builtin.com/data-science/wavelet-transform},
  journal = {Built In},
  author  = {Talebi, Shawhin}
}


@inproceedings{wavelet_denoise,
  author    = {Dautov, Çiğdem Polat and Özerdem, Mehmet Siraç},
  booktitle = {2018 26th Signal Processing and Communications Applications Conference (SIU)},
  title     = {Wavelet transform and signal denoising using Wavelet method},
  year      = {2018},
  pages     = {1-4},
  doi       = {10.1109/SIU.2018.8404418}
}


@misc{weale2018,
  title     = {Music disappearing from curriculum, schools survey shows},
  url       = {https://www.theguardian.com/education/2018/oct/10/music-disappearing-school-curriculum-england-survey-gcse-a-level},
  journal   = {The Guardian},
  publisher = {Guardian News and Media},
  author    = { Weale, Sally},
  year      = {2018},
  month     = {Oct}
}


@misc{weale2021,
  title     = {'Creativity crisis' looms for English schools due to arts cuts, says Labour},
  url       = {https://www.theguardian.com/education/2021/jul/15/creativity-crisis-looms-for-english-schools-due-to-arts-cuts-says-labour},
  journal   = {The Guardian},
  publisher = {Guardian News and Media},
  author    = { Weale, Sally},
  year      = {2021},
  month     = {Jul}
}

@article{womenprange,
  author  = {Robert E. McGlone  and Harry Hollien },
  title   = {Vocal Pitch Characteristics of Aged Women},
  journal = {Journal of Speech and Hearing Research},
  volume  = {6},
  number  = {2},
  pages   = {164-170},
  year    = {1963},
  doi     = {10.1044/jshr.0602.164},
  url     = {https://pubs.asha.org/doi/abs/10.1044/jshr.0602.164},
  eprint  = {https://pubs.asha.org/doi/pdf/10.1044/jshr.0602.164}
} 

@misc{worldwide2020,
  title     = {The Independent Artist - Music Business Worldwide 2020},
  url       = {https://www.musicbusinessworldwide.com/files/2020/03/The-Independent-Artist-2020.03.14-vEXTERNAL.pdf},
  publisher = {Raine},
  year      = {2020},
  month     = {Mar}
} 

@article{yin,
  title   = {Yin, a fundamental frequency estimator for speech and music},
  volume  = {111},
  doi     = {10.1121/1.1458024},
  number  = {4},
  journal = {The Journal of the Acoustical Society of America},
  author  = {de Cheveigné, Alain and Kawahara, Hideki},
  year    = {2002},
  pages   = {1917-1930}
} 

@misc{YouGovMR,
  title     = {The Musical Instruments You Play},
  url       = {https://yougov.co.uk/topics/entertainment/articles-reports/2011/11/01/musical-instruments-you-play},
  journal   = {YouGov},
  publisher = {YouGov},
  author    = {Blacklock, Daisy},
  year      = {2011},
  month     = {Nov}
} 

@misc{zcr,
  title     = {Algorithmic frequency/pitch detection series - part 1: Making a simple pitch tracker using zero...},
  url       = {https://medium.com/the-seekers-project/algorithmic-frequency-pitch-detection-series-part-1-making-a-simple-pitch-tracker-using-zero-9991327897a4},
  journal   = {Medium},
  publisher = {The Seeker's Project},
  author    = {Daoo, Rishikesh},
  year      = {2020},
  month     = {May}
} 

@article{ExperiencedListeners,
  author  = {Bigand, E and Poulin-Charronnat, Bénédicte},
  year    = {2006},
  month   = {06},
  pages   = {100-30},
  title   = {Are we “Experienced listeners?” A review of the musical capacities that do not depend on formal musical training},
  volume  = {100},
  journal = {Cognition},
  doi     = {10.1016/j.cognition.2005.11.007}
}

@article{UnreasonableEffectivenessOfData,
  author    = {F. Pereira and P. Norvig and A. Halevy},
  journal   = {IEEE Intelligent Systems},
  title     = {The Unreasonable Effectiveness of Data},
  year      = {2009},
  volume    = {24},
  number    = {02},
  issn      = {1941-1294},
  pages     = {8-12},
  keywords  = {machine learning;very large data bases;semantic web},
  doi       = {10.1109/MIS.2009.36},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  month     = {mar}
}
