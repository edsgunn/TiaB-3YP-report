\documentclass[11pt]{article}
\usepackage[a4paper, margin=20mm]{geometry}

\usepackage[utf8]{inputenc}
\sffamily

%\title{3YP}
%\author{Terence Tan}
%\date{March 2022}

\begin{document}

%\maketitle

\section{Data}

\subsection{Dataset}
The dataset \cite{dataset} used to train and test the machine learning model was obtained from another paper \cite{dataset_paper} that was working on a similar project. The authors of that paper made their dataset available online. There are 2252 songs in this dataset, 1802 of which had been categorised as the training set while the rest had been categorised as the test set. Each song is in major key and only have a single chord per bar. For each song, all the relevant features had been extracted and placed into a single \emph{CSV} file as shown in Figure.

As can be seen in Figure, the rows each contain information about a single note. Each bar is taken to be a single measure. The columns each represent a different piece of information about that particular note. \emph{time} refers to the time signature, \emph{measure} refers to the measure to which that particular note belongs to, \emph{key\_fifths} indicates the number of sharps/flats (e.g. -1 for one flat and 1 for one sharp), \emph{chord\_root} is the root of the chord with \emph{chord\_type} indicating the type of chord, \emph{note\_root} identifies the particular single note of that row, \emph{note\_octave} is the octave of that note, and \emph{note\_duration} indicated the duration of the note (4.0 for a quarter note).

\subsection{Preprocessing of dataset}
The dataset has to be preprocessed in order to make things simpler later on.

\begin{enumerate}
  \item All songs are transposed to C major key. The key of a song determines the notes and the set of chords present in the song. Transposing all songs to a common key will basically normalise the different features of melodies and chords in different songs. The number of chord types present in the dataset will be reduced, which will decrease the number of chord types during the training process. Each song can be shifted to a different key without loss of the song's subjective character \cite{paper2}.
  \item The time signatures are all normalised. Different songs have different time signatures. To do so, each \emph{note\_duration} is multiplied by the reciprocal of the time signature \emph{time} to give a normalised note duration.
  \item Chord types are restricted to C major and C minor chords. All other chord types are converted to their most similar scale tone chords in key C.
  \item Some measures in the dataset contain rest notes. These measures are removed from the dataset.
  \item Octave information is not required and is removed from the dataset.
\end{enumerate}


\begin{thebibliography}{9}

   \bibitem{dataset}
    H. Lim, S. Ryu, and K. Lee, “CSV Leadsheet Database,” CSV\_Leadsheet\_DB. [Online]. Available: http://marg.snu.ac.kr/chord\_generation/. [Accessed: 05-Jan-2022]. 
   
    \bibitem{dataset_paper}
    Hyungui Lim, Seungyeon Ryu and Kyogu Lee. “Chord Generation from Symbolic Melody Using BLSTM Networks”, 18th International Society for Music Information Retrieval Conference, 2017
    
    \bibitem{paper2}
    Chen, Ziheng and Jie Qi. “Machine Learning in Automatic Music Chords Generation.” (2015).

    \end{thebibliography}
\end{document}
